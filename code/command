imageclass.py and transformer.py: run on a single GPU to collect trace and print time information.
imageclass_ddp.py and transformer_ddp.py: run on multi-GPUs to collect trace and print time information.


mkdir imageclass_ddp_profiler
mkdir imageclass_profiler
mkdir transformer_ddp_profiler
mkdir transformer_profiler

load ILSVRC2012_img_val to ./

python imageclass.py 128
python transformer.py 128

execute imageclass_ddp.py and transformer_ddp.py on 8 GPUs; batch size set: 8 times of batch size on single GPU, which is 1024


---------------------------------------------
for tensor parallel case:

pip install tensor_parallel

mkdir imageclass_tp_profiler
mkdir transformer_tp_profiler

imageclass_tp.py and transformer_tp.py: run on multi-GPUs to collect trace and print time information for tensor parallel.
execute imageclass_tp.py and transformer_tp.py on 8 GPUs; batch size set: 8 times of batch size on single GPU, which is 1024
note that: it needs two parameters, the 1st parameter is the batch size, and the second parameter, please set it to be 1 and 2, separately.
for example on a single node: torchrun --standalone --nproc_per_node=2 transformer_tp.py 1024 1


