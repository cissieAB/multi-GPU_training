imageclass.py and transformer.py: run on a single GPU to collect trace and print time information.
imageclass_ddp.py and transformer_ddp.py: run on multi-GPUs to collect trace and print time information.


mkdir imageclass_ddp_profiler
mkdir imageclass_profiler
mkdir transformer_ddp_profiler
mkdir transformer_profiler

load ILSVRC2012_img_val to ./

python imageclass.py 128
python transformer.py 128

execute imageclass_ddp.py and transformer_ddp.py on 8 GPUs; batch size set: 8 times of batch size on single GPU, which is 1024


---------------------------------------------
for tensor parallel case:

pip install tensor_parallel

mkdir imageclass_tp_profiler
mkdir transformer_tp_profiler

imageclass_tp.py and transformer_tp.py: run on multi-GPUs to collect trace and print time information for tensor parallel.
execute imageclass_tp.py and transformer_tp.py on 8 GPUs; batch size set: 8 times of batch size on single GPU, which is 1024
note that: it needs two parameters, the 1st parameter is the batch size, and the second parameter, please set it to be 1 and 2, separately.
for example on a single node: torchrun --standalone --nproc_per_node=2 transformer_tp.py 1024 1


---------------------------------------------
for llama, we'd like to collect traces of llama model for the previous two cases, data parallel and tensor parallel. 
so the codes are similar to the previous _tp.py and _ddp.py codes.


when running the code, if the line, "AutoModelForCausalLM.from_pretrained(name)" returns the error:
ValueError: rope_scaling must be a dictionary with two fields, type and factor, got {'factor': 32.0, 'high_freq_factor': 4.0, 'low_freq_factor': 1.0, 'original_max_position_embeddings': 8192, 'rope_type': 'llama3'}
we can check the transformers version (pip show transformers)
and make sure transformers >= 4.43.0 (pip install --upgrade transformers)

mkdir llama_profiler

for llama_trainex_ddp.py, we only have one parameter, batch size:
torchrun --standalone --nproc_per_node=2 llama_trainex_ddp.py 1024

for llama_trainex_tp.py, we have two parameters, batch size and distributed case(set it to 1 and 2, separately ):
torchrun --standalone --nproc_per_node=2 llama_trainex_ddp.py 1024 1
torchrun --standalone --nproc_per_node=2 llama_trainex_ddp.py 1024 2


