+ env
+ grep -i slurm
SLURM_JOB_USER=xmei
SLURM_TASKS_PER_NODE=1
SLURM_JOB_UID=11066
SLURM_LUSTRE_JOB_ID=sciml2301,xmei,31535138
SLURM_TASK_PID=1642520
SLURM_JOB_GPUS=0,1,2,3
SLURM_LOCALID=0
SLURM_SUBMIT_DIR=/w/epsci-sciwork18/xmei/projects/yifan_sun
SLURMD_NODENAME=sciml2301
SLURM_JOB_START_TIME=1729694665
SLURM_CLUSTER_NAME=scicomp
SLURM_JOB_END_TIME=1729709065
SLURM_CPUS_ON_NODE=4
SLURM_JOB_CPUS_PER_NODE=4
SLURM_GPUS_ON_NODE=4
PRTE_MCA_plm_slurm_args=--external-launcher
SLURM_GTIDS=0
SLURM_JOB_PARTITION=gpu
SLURM_TRES_PER_TASK=cpu:4
SLURM_JOB_NUM_NODES=1
SLURM_JOBID=31535138
SLURM_JOB_QOS=normal
SLURM_PROCID=0
TMPDIR=/scratch/slurm/31535138/.cache/tmp
SLURM_CPUS_PER_TASK=4
SLURM_TOPOLOGY_ADDR=sciml2301
HYDRA_BOOTSTRAP=slurm
SLURM_TOPOLOGY_ADDR_PATTERN=node
SLURM_MEM_PER_CPU=8000
SLURM_SCRIPT_CONTEXT=prolog_task
SLURM_NODELIST=sciml2301
SLURM_JOB_ACCOUNT=epsci
SLURM_PRIO_PROCESS=0
SLURM_NNODES=1
SLURM_SUBMIT_HOST=ifarm2402.jlab.org
XDG_RUNTIME_DIR=/scratch/slurm/31535138/.cache/run
SLURM_JOB_ID=31535138
SLURM_NODEID=0
SLURM_CONF=/etc/slurm/slurm.conf
SLURM_JOB_NAME=1-node_gpt
OMPI_MCA_plm_slurm_args=--external-launcher
SLURM_JOB_GID=761
SLURM_JOB_NODELIST=sciml2301
I_MPI_HYDRA_BOOTSTRAP=slurm
+ env
+ grep -i rank
+ env
+ grep -i cuda
CUDA_VISIBLE_DEVICES=0,1,2,3
+ env
+ grep -i nccl
+ echo -e '=============================================================\n\n'
=============================================================


+ srun --job-name hostname --nodes 1 --ntasks-per-node 1 hostname
sciml2301.jlab.org
++ scontrol show hostnames sciml2301
++ head -n 1
+ export MASTER_ADDR=sciml2301
+ MASTER_ADDR=sciml2301
+ export MASTER_PORT=32800
+ MASTER_PORT=32800
+ echo Head Node: sciml2301:32800
Head Node: sciml2301:32800
+ echo -e '=============================================================\n\n'
=============================================================


+ export PATH=/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/puppetlabs/bin:/home/xmei/projects/iperf3/bin:/home/xmei/projects/py3.10/bin:/work/epsci/xmei/projects/yifan_sun/py-torch/bin
+ PATH=/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/puppetlabs/bin:/home/xmei/projects/iperf3/bin:/home/xmei/projects/py3.10/bin:/work/epsci/xmei/projects/yifan_sun/py-torch/bin
+ export LOGLEVEL=INFO
+ LOGLEVEL=INFO
+ export NCCL_DEBUG=INFO
+ NCCL_DEBUG=INFO
+ export TORCH_DISTRIBUTED_DEBUG=INFO
+ TORCH_DISTRIBUTED_DEBUG=INFO
+ export CUDA_VISIBLE_DEVICES=0,1,2,3
+ CUDA_VISIBLE_DEVICES=0,1,2,3
+ export 'PYTHON_DIST_JOB_ARGS=-m torch.distributed.run --nproc_per_node=4 --nnodes=1 --master-addr  --master-port '
+ PYTHON_DIST_JOB_ARGS='-m torch.distributed.run --nproc_per_node=4 --nnodes=1 --master-addr  --master-port '
++ hostname
+ srun --job-name print-cuda --nodes 1 --ntasks-per-node 1 echo sciml2301.jlab.org 0,1,2,3
sciml2301.jlab.org 0,1,2,3
+ echo -e '=============================================================\n\n'
=============================================================


+ srun torchrun --nproc_per_node=4 --rdzv_backend=c10d --rdzv_endpoint=sciml2301.jlab.org:32800 --nnodes=1 --rdzv-id 6536 transformer_ddp.py 256

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.1.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/bin/torchrun", line 5, in <module>
    from torch.distributed.run import main
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/__init__.py", line 1382, in <module>
    from .functional import *  # noqa: F403
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/functional.py", line 7, in <module>
    import torch.nn.functional as F
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/nn/__init__.py", line 1, in <module>
    from .modules import *  # noqa: F403
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/nn/modules/__init__.py", line 35, in <module>
    from .transformer import TransformerEncoder, TransformerDecoder, \
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/nn/modules/transformer.py", line 20, in <module>
    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)
  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
[2024-10-23 10:44:34,773] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-10-23 10:44:34,773] torch.distributed.run: [WARNING] 
[2024-10-23 10:44:34,773] torch.distributed.run: [WARNING] *****************************************
[2024-10-23 10:44:34,773] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-10-23 10:44:34,773] torch.distributed.run: [WARNING] *****************************************
[2024-10-23 10:44:34,774] torch.distributed.launcher.api: [INFO] Starting elastic_operator with launch configs:
[2024-10-23 10:44:34,774] torch.distributed.launcher.api: [INFO]   entrypoint       : transformer_ddp.py
[2024-10-23 10:44:34,774] torch.distributed.launcher.api: [INFO]   min_nodes        : 1
[2024-10-23 10:44:34,774] torch.distributed.launcher.api: [INFO]   max_nodes        : 1
[2024-10-23 10:44:34,774] torch.distributed.launcher.api: [INFO]   nproc_per_node   : 4
[2024-10-23 10:44:34,774] torch.distributed.launcher.api: [INFO]   run_id           : 6536
[2024-10-23 10:44:34,774] torch.distributed.launcher.api: [INFO]   rdzv_backend     : c10d
[2024-10-23 10:44:34,774] torch.distributed.launcher.api: [INFO]   rdzv_endpoint    : sciml2301.jlab.org:32800
[2024-10-23 10:44:34,774] torch.distributed.launcher.api: [INFO]   rdzv_configs     : {'timeout': 900}
[2024-10-23 10:44:34,774] torch.distributed.launcher.api: [INFO]   max_restarts     : 0
[2024-10-23 10:44:34,774] torch.distributed.launcher.api: [INFO]   monitor_interval : 5
[2024-10-23 10:44:34,774] torch.distributed.launcher.api: [INFO]   log_dir          : None
[2024-10-23 10:44:34,774] torch.distributed.launcher.api: [INFO]   metrics_cfg      : {}
[2024-10-23 10:44:34,774] torch.distributed.launcher.api: [INFO] 
[2024-10-23 10:44:34,808] torch.distributed.elastic.agent.server.local_elastic_agent: [INFO] log directory set to: /scratch/slurm/31535138/.cache/tmp/torchelastic_03pjkcrt/6536_osznb5x3
[2024-10-23 10:44:34,808] torch.distributed.elastic.agent.server.api: [INFO] [default] starting workers for entrypoint: python3.10
[2024-10-23 10:44:34,808] torch.distributed.elastic.agent.server.api: [INFO] [default] Rendezvous'ing worker group
[2024-10-23 10:44:34,946] torch.distributed.elastic.agent.server.api: [INFO] [default] Rendezvous complete for workers. Result:
[2024-10-23 10:44:34,946] torch.distributed.elastic.agent.server.api: [INFO]   restart_count=0
[2024-10-23 10:44:34,946] torch.distributed.elastic.agent.server.api: [INFO]   master_addr=sciml2301.jlab.org
[2024-10-23 10:44:34,946] torch.distributed.elastic.agent.server.api: [INFO]   master_port=38125
[2024-10-23 10:44:34,946] torch.distributed.elastic.agent.server.api: [INFO]   group_rank=0
[2024-10-23 10:44:34,946] torch.distributed.elastic.agent.server.api: [INFO]   group_world_size=1
[2024-10-23 10:44:34,946] torch.distributed.elastic.agent.server.api: [INFO]   local_ranks=[0, 1, 2, 3]
[2024-10-23 10:44:34,946] torch.distributed.elastic.agent.server.api: [INFO]   role_ranks=[0, 1, 2, 3]
[2024-10-23 10:44:34,946] torch.distributed.elastic.agent.server.api: [INFO]   global_ranks=[0, 1, 2, 3]
[2024-10-23 10:44:34,946] torch.distributed.elastic.agent.server.api: [INFO]   role_world_sizes=[4, 4, 4, 4]
[2024-10-23 10:44:34,946] torch.distributed.elastic.agent.server.api: [INFO]   global_world_sizes=[4, 4, 4, 4]
[2024-10-23 10:44:34,946] torch.distributed.elastic.agent.server.api: [INFO] 
[2024-10-23 10:44:34,946] torch.distributed.elastic.agent.server.api: [INFO] [default] Starting worker group
[2024-10-23 10:44:34,947] torch.distributed.elastic.agent.server.local_elastic_agent: [INFO] Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
[2024-10-23 10:44:34,947] torch.distributed.elastic.multiprocessing: [INFO] Setting worker0 reply file to: /scratch/slurm/31535138/.cache/tmp/torchelastic_03pjkcrt/6536_osznb5x3/attempt_0/0/error.json
[2024-10-23 10:44:34,947] torch.distributed.elastic.multiprocessing: [INFO] Setting worker1 reply file to: /scratch/slurm/31535138/.cache/tmp/torchelastic_03pjkcrt/6536_osznb5x3/attempt_0/1/error.json
[2024-10-23 10:44:34,947] torch.distributed.elastic.multiprocessing: [INFO] Setting worker2 reply file to: /scratch/slurm/31535138/.cache/tmp/torchelastic_03pjkcrt/6536_osznb5x3/attempt_0/2/error.json
[2024-10-23 10:44:34,947] torch.distributed.elastic.multiprocessing: [INFO] Setting worker3 reply file to: /scratch/slurm/31535138/.cache/tmp/torchelastic_03pjkcrt/6536_osznb5x3/attempt_0/3/error.json

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.1.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/transformer_ddp.py", line 4, in <module>
    import torch
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/__init__.py", line 1382, in <module>
    from .functional import *  # noqa: F403
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/functional.py", line 7, in <module>
    import torch.nn.functional as F
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/nn/__init__.py", line 1, in <module>
    from .modules import *  # noqa: F403
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/nn/modules/__init__.py", line 35, in <module>
    from .transformer import TransformerEncoder, TransformerDecoder, \
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/nn/modules/transformer.py", line 20, in <module>
    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)
  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.1.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/transformer_ddp.py", line 4, in <module>
    import torch
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/__init__.py", line 1382, in <module>
    from .functional import *  # noqa: F403
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/functional.py", line 7, in <module>
    import torch.nn.functional as F
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/nn/__init__.py", line 1, in <module>
    from .modules import *  # noqa: F403
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/nn/modules/__init__.py", line 35, in <module>
    from .transformer import TransformerEncoder, TransformerDecoder, \
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/nn/modules/transformer.py", line 20, in <module>
    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)
  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.1.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/transformer_ddp.py", line 4, in <module>
    import torch
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/__init__.py", line 1382, in <module>
    from .functional import *  # noqa: F403
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/functional.py", line 7, in <module>
    import torch.nn.functional as F
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/nn/__init__.py", line 1, in <module>
    from .modules import *  # noqa: F403
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/nn/modules/__init__.py", line 35, in <module>
    from .transformer import TransformerEncoder, TransformerDecoder, \
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/nn/modules/transformer.py", line 20, in <module>
    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)
  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.1.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/transformer_ddp.py", line 4, in <module>
    import torch
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/__init__.py", line 1382, in <module>
    from .functional import *  # noqa: F403
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/functional.py", line 7, in <module>
    import torch.nn.functional as F
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/nn/__init__.py", line 1, in <module>
    from .modules import *  # noqa: F403
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/nn/modules/__init__.py", line 35, in <module>
    from .transformer import TransformerEncoder, TransformerDecoder, \
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/nn/modules/transformer.py", line 20, in <module>
    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)
  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
Hostname: sciml2301.jlab.org, Rank: 3, Local Rank: 3, Global Rank: 3, NUM_GPS: 4
Hostname: sciml2301.jlab.org, Rank: 0, Local Rank: 0, Global Rank: 0, NUM_GPS: 4
[sciml2301.jlab.org] Rank 0, Local Rank 0: CUDA device set to 0
Hostname: sciml2301.jlab.org, Rank: 2, Local Rank: 2, Global Rank: 2, NUM_GPS: 4
Hostname: sciml2301.jlab.org, Rank: 1, Local Rank: 1, Global Rank: 1, NUM_GPS: 4
[sciml2301.jlab.org] Rank 3, Local Rank 3: CUDA device set to 3
[sciml2301.jlab.org] Rank 2, Local Rank 2: CUDA device set to 2
[sciml2301.jlab.org] Rank 1, Local Rank 1: CUDA device set to 1
Dataset loaded.
training model now: gpt2
Dataset loaded.
training model now: gpt2
Dataset loaded.
training model now: gpt2
Dataset loaded.
training model now: gpt2
sciml2301:1642595:1642595 [0] NCCL INFO Bootstrap : Using ibp37s0:172.17.1.12<0>
sciml2301:1642595:1642595 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
sciml2301:1642595:1642595 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
sciml2301:1642595:1642595 [0] NCCL INFO cudaDriverVersion 12060
NCCL version 2.18.1+cuda12.1
sciml2301:1642596:1642596 [1] NCCL INFO cudaDriverVersion 12060
sciml2301:1642596:1642596 [1] NCCL INFO Bootstrap : Using ibp37s0:172.17.1.12<0>
sciml2301:1642597:1642597 [2] NCCL INFO cudaDriverVersion 12060
sciml2301:1642596:1642596 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
sciml2301:1642596:1642596 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
sciml2301:1642597:1642597 [2] NCCL INFO Bootstrap : Using ibp37s0:172.17.1.12<0>
sciml2301:1642597:1642597 [2] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
sciml2301:1642597:1642597 [2] NCCL INFO NET/Plugin : No plugin found, using internal implementation
sciml2301:1642598:1642598 [3] NCCL INFO cudaDriverVersion 12060
sciml2301:1642598:1642598 [3] NCCL INFO Bootstrap : Using ibp37s0:172.17.1.12<0>
sciml2301:1642598:1642598 [3] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
sciml2301:1642598:1642598 [3] NCCL INFO NET/Plugin : No plugin found, using internal implementation
sciml2301:1642597:1642679 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibp37s0:172.17.1.12<0>
sciml2301:1642597:1642679 [2] NCCL INFO Using network IB
sciml2301:1642597:1642679 [2] NCCL INFO DMA-BUF is available on GPU device 2
sciml2301:1642598:1642680 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibp37s0:172.17.1.12<0>
sciml2301:1642598:1642680 [3] NCCL INFO Using network IB
sciml2301:1642598:1642680 [3] NCCL INFO DMA-BUF is available on GPU device 3
sciml2301:1642595:1642677 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibp37s0:172.17.1.12<0>
sciml2301:1642595:1642677 [0] NCCL INFO Using network IB
sciml2301:1642595:1642677 [0] NCCL INFO DMA-BUF is available on GPU device 0
sciml2301:1642596:1642678 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibp37s0:172.17.1.12<0>
sciml2301:1642596:1642678 [1] NCCL INFO Using network IB
sciml2301:1642596:1642678 [1] NCCL INFO DMA-BUF is available on GPU device 1
sciml2301:1642597:1642679 [2] NCCL INFO NVLS multicast support is not available on dev 2
sciml2301:1642595:1642677 [0] NCCL INFO NVLS multicast support is not available on dev 0
sciml2301:1642596:1642678 [1] NCCL INFO NVLS multicast support is not available on dev 1
sciml2301:1642598:1642680 [3] NCCL INFO NVLS multicast support is not available on dev 3
sciml2301:1642597:1642679 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
sciml2301:1642598:1642680 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
sciml2301:1642598:1642680 [3] NCCL INFO P2P Chunksize set to 524288
sciml2301:1642596:1642678 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
sciml2301:1642596:1642678 [1] NCCL INFO P2P Chunksize set to 524288
sciml2301:1642595:1642677 [0] NCCL INFO Channel 00/02 :    0   1   2   3
sciml2301:1642595:1642677 [0] NCCL INFO Channel 01/02 :    0   1   2   3
sciml2301:1642595:1642677 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
sciml2301:1642595:1642677 [0] NCCL INFO P2P Chunksize set to 524288
sciml2301:1642597:1642679 [2] NCCL INFO P2P Chunksize set to 524288
sciml2301:1642595:1642677 [0] NCCL INFO Channel 00/0 : 0[83000] -> 1[84000] via P2P/IPC/read
sciml2301:1642597:1642679 [2] NCCL INFO Channel 00/0 : 2[c3000] -> 3[c4000] via P2P/IPC/read
sciml2301:1642596:1642678 [1] NCCL INFO Channel 00/0 : 1[84000] -> 2[c3000] via P2P/IPC
sciml2301:1642598:1642680 [3] NCCL INFO Channel 00/0 : 3[c4000] -> 0[83000] via P2P/IPC
sciml2301:1642595:1642677 [0] NCCL INFO Channel 01/0 : 0[83000] -> 1[84000] via P2P/IPC/read
sciml2301:1642597:1642679 [2] NCCL INFO Channel 01/0 : 2[c3000] -> 3[c4000] via P2P/IPC/read
sciml2301:1642596:1642678 [1] NCCL INFO Channel 01/0 : 1[84000] -> 2[c3000] via P2P/IPC
sciml2301:1642598:1642680 [3] NCCL INFO Channel 01/0 : 3[c4000] -> 0[83000] via P2P/IPC
sciml2301:1642595:1642677 [0] NCCL INFO Connected all rings
sciml2301:1642596:1642678 [1] NCCL INFO Connected all rings
sciml2301:1642598:1642680 [3] NCCL INFO Connected all rings
sciml2301:1642598:1642680 [3] NCCL INFO Channel 00/0 : 3[c4000] -> 2[c3000] via P2P/IPC/read
sciml2301:1642597:1642679 [2] NCCL INFO Connected all rings
sciml2301:1642598:1642680 [3] NCCL INFO Channel 01/0 : 3[c4000] -> 2[c3000] via P2P/IPC/read
sciml2301:1642597:1642679 [2] NCCL INFO Channel 00/0 : 2[c3000] -> 1[84000] via P2P/IPC
sciml2301:1642596:1642678 [1] NCCL INFO Channel 00/0 : 1[84000] -> 0[83000] via P2P/IPC/read
sciml2301:1642597:1642679 [2] NCCL INFO Channel 01/0 : 2[c3000] -> 1[84000] via P2P/IPC
sciml2301:1642596:1642678 [1] NCCL INFO Channel 01/0 : 1[84000] -> 0[83000] via P2P/IPC/read
sciml2301:1642598:1642680 [3] NCCL INFO Connected all trees
sciml2301:1642598:1642680 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
sciml2301:1642598:1642680 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
sciml2301:1642595:1642677 [0] NCCL INFO Connected all trees
sciml2301:1642595:1642677 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
sciml2301:1642595:1642677 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
sciml2301:1642597:1642679 [2] NCCL INFO Connected all trees
sciml2301:1642597:1642679 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
sciml2301:1642597:1642679 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
sciml2301:1642596:1642678 [1] NCCL INFO Connected all trees
sciml2301:1642596:1642678 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
sciml2301:1642596:1642678 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
sciml2301:1642595:1642677 [0] NCCL INFO comm 0x13862130 rank 0 nranks 4 cudaDev 0 busId 83000 commId 0x8108fb0a88a09816 - Init COMPLETE
sciml2301:1642597:1642679 [2] NCCL INFO comm 0x10937790 rank 2 nranks 4 cudaDev 2 busId c3000 commId 0x8108fb0a88a09816 - Init COMPLETE
sciml2301:1642598:1642680 [3] NCCL INFO comm 0xf001130 rank 3 nranks 4 cudaDev 3 busId c4000 commId 0x8108fb0a88a09816 - Init COMPLETE
sciml2301:1642596:1642678 [1] NCCL INFO comm 0xe547db0 rank 1 nranks 4 cudaDev 1 busId 84000 commId 0x8108fb0a88a09816 - Init COMPLETE
gpt2gpt2 gpu time 2 gpt2487.4915771484375
gpt2 gpu time 2  487.4670104980469gpu time 2
 487.4844055175781
 gpu time 2 487.4198913574219
gpt2 gpu time 2gpt2 489.5528869628906gpt2
 gpu time 2 489.4422912597656
gpt2 gpu time 2 489.5897521972656
 gpu time 2 489.45562744140625
gpt2 gpu time 2 488.922119140625gpt2
 gpu time 2 gpt2488.911865234375
 gpu time 2 488.95794677734375
gpt2 gpu time 2 488.7879638671875
gpt2gpt2gpt2  gpu time 2gpu time 2  488.9753723144531489.1852722167969 
gpu time 2 489.18731689453125
gpt2 gpu time 2 489.2641296386719

gpt2gpt2  gpu time 2gpu time 2  489.7914733886719489.83551025390625gpt2

 gpu time 2 489.8252868652344
gpt2 gpu time 2 489.6645202636719
gpt2 gpu time 2 487.9308776855469
gpt2 gpu time 2 487.952392578125
gpt2 gpu time 2 487.96978759765625
gpt2 gpu time 2 487.7025146484375
gpt2gpt2 gpu time 2  gpt2gpu time 2  gpu time 2489.1115417480469489.0705871582031 

489.0245056152344
gpt2 gpu time 2 488.8330383300781
gpt2gpt2 gpu time 2 488.68353271484375
gpt2 gpu time 2 488.7357482910156
gpt2 gpu time 2 488.52276611328125
 gpu time 2 488.690673828125
gpt2gpt2 gpu time 2 490.1795959472656
gpt2 gpu time 2 490.113037109375
gpt2 gpu time 2 490.20416259765625
 gpu time 2 490.0157470703125
gpt2gpt2 gpu time 2 489.5027160644531 gpu time 2 gpt2489.3102111816406 
gpu time 2gpt2  489.5119323730469gpu time 2 
489.5467529296875

STAGE:2024-10-23 10:45:16 1642598:1642598 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-10-23 10:45:16 1642595:1642595 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-10-23 10:45:16 1642597:1642597 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-10-23 10:45:16 1642596:1642596 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-10-23 10:45:17 1642598:1642598 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-23 10:45:17 1642596:1642596 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-23 10:45:17 1642597:1642597 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-23 10:45:17 1642595:1642595 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-23 10:45:17 1642596:1642596 ActivityProfilerController.cpp:322] Completed Stage: Post ProcessingSTAGE:2024-10-23 10:45:17 1642598:1642598 ActivityProfilerController.cpp:322] Completed Stage: Post Processing

STAGE:2024-10-23 10:45:17 1642597:1642597 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2024-10-23 10:45:17 1642595:1642595 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
gpt2gpt2 gpu time 519.1968994140625
gpt2 gpu time 518.962646484375 gpu timegpt2  507.0950012207031gpu time
 505.7196044921875

STAGE:2024-10-23 10:45:18 1642596:1642596 ActivityProfilerController.cpp:312] Completed Stage: Warm UpSTAGE:2024-10-23 10:45:18 1642598:1642598 ActivityProfilerController.cpp:312] Completed Stage: Warm Up

STAGE:2024-10-23 10:45:18 1642595:1642595 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-10-23 10:45:18 1642597:1642597 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-10-23 10:45:18 1642596:1642596 ActivityProfilerController.cpp:318] Completed Stage: CollectionSTAGE:2024-10-23 10:45:18 1642597:1642597 ActivityProfilerController.cpp:318] Completed Stage: Collection

STAGE:2024-10-23 10:45:18 1642595:1642595 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-23 10:45:18 1642598:1642598 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-23 10:45:18 1642597:1642597 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2024-10-23 10:45:18 1642596:1642596 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2024-10-23 10:45:18 1642595:1642595 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2024-10-23 10:45:18 1642598:1642598 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
gpt2 gpu time 504.467529296875
gpt2 gpu time 504.3587646484375
gpt2 gpu time 504.46563720703125
gpt2 gpu time 504.277099609375
STAGE:2024-10-23 10:45:19 1642595:1642595 ActivityProfilerController.cpp:312] Completed Stage: Warm UpSTAGE:2024-10-23 10:45:19 1642597:1642597 ActivityProfilerController.cpp:312] Completed Stage: Warm Up

STAGE:2024-10-23 10:45:19 1642596:1642596 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-10-23 10:45:19 1642598:1642598 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-10-23 10:45:20 1642596:1642596 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-23 10:45:20 1642598:1642598 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-23 10:45:20 1642597:1642597 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-23 10:45:20 1642595:1642595 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-23 10:45:20 1642596:1642596 ActivityProfilerController.cpp:322] Completed Stage: Post ProcessingSTAGE:2024-10-23 10:45:20 1642598:1642598 ActivityProfilerController.cpp:322] Completed Stage: Post Processing

STAGE:2024-10-23 10:45:20 1642595:1642595 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2024-10-23 10:45:20 1642597:1642597 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
gpt2gpt2 gpu time 503.8990173339844
 gpu time 504.04705810546875
STAGE:2024-10-23 10:45:20 1642598:1642598 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-10-23 10:45:20 1642595:1642595 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
gpt2gpt2 gpu time 504.02178955078125
 gpu time 504.2809143066406
STAGE:2024-10-23 10:45:21 1642596:1642596 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-10-23 10:45:21 1642597:1642597 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-10-23 10:45:21 1642596:1642596 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-23 10:45:21 1642597:1642597 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-23 10:45:21 1642595:1642595 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-23 10:45:21 1642598:1642598 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-23 10:45:21 1642596:1642596 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2024-10-23 10:45:21 1642597:1642597 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2024-10-23 10:45:21 1642595:1642595 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2024-10-23 10:45:21 1642598:1642598 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
gpt2gpt2 gpu time 506.3482360839844
 gpu time 506.5402526855469
STAGE:2024-10-23 10:45:22 1642597:1642597 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-10-23 10:45:22 1642596:1642596 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
gpt2gpt2 gpu time 602.7550048828125
 gpu time 602.8532104492188
STAGE:2024-10-23 10:45:22 1642595:1642595 ActivityProfilerController.cpp:312] Completed Stage: Warm UpSTAGE:2024-10-23 10:45:22 1642598:1642598 ActivityProfilerController.cpp:312] Completed Stage: Warm Up

STAGE:2024-10-23 10:45:23 1642598:1642598 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-23 10:45:23 1642595:1642595 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-23 10:45:23 1642597:1642597 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-23 10:45:23 1642596:1642596 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-23 10:45:23 1642598:1642598 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2024-10-23 10:45:23 1642595:1642595 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2024-10-23 10:45:23 1642596:1642596 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2024-10-23 10:45:23 1642597:1642597 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
gpt2gpt2 gpu time 502.21484375
 gpu time 502.3358459472656
STAGE:2024-10-23 10:45:23 1642595:1642595 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-10-23 10:45:23 1642598:1642598 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
gpt2gpt2 gpu time 672.496826171875 gpu time
 672.4906005859375
STAGE:2024-10-23 10:45:24 1642596:1642596 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-10-23 10:45:24 1642597:1642597 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-10-23 10:45:24 1642596:1642596 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-23 10:45:24 1642597:1642597 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-23 10:45:24 1642595:1642595 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-23 10:45:24 1642598:1642598 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-23 10:45:24 1642596:1642596 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2024-10-23 10:45:24 1642597:1642597 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2024-10-23 10:45:24 1642595:1642595 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2024-10-23 10:45:24 1642598:1642598 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
gpt2gpt2 gpu time 506.2189636230469
 gpu time 504.9792175292969
STAGE:2024-10-23 10:45:25 1642596:1642596 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-10-23 10:45:25 1642597:1642597 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
gpt2 gpu time 598.9052124023438
STAGE:2024-10-23 10:45:25 1642595:1642595 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
gpt2 gpu time 598.8724365234375
STAGE:2024-10-23 10:45:25 1642598:1642598 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-10-23 10:45:26 1642598:1642598 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-23 10:45:26 1642595:1642595 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-23 10:45:26 1642596:1642596 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-23 10:45:26 1642597:1642597 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-23 10:45:26 1642598:1642598 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2024-10-23 10:45:26 1642595:1642595 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2024-10-23 10:45:26 1642596:1642596 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2024-10-23 10:45:26 1642597:1642597 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
gpt2gpt2 gpu time 503.6885070800781
gpt2 gpu time 545.119140625
 gpu time 501.84503173828125
gpt2 gpu time 531.3444213867188
STAGE:2024-10-23 10:45:27 1642596:1642596 ActivityProfilerController.cpp:312] Completed Stage: Warm UpSTAGE:2024-10-23 10:45:27 1642595:1642595 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-10-23 10:45:27 1642598:1642598 ActivityProfilerController.cpp:312] Completed Stage: Warm Up

STAGE:2024-10-23 10:45:27 1642597:1642597 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-10-23 10:45:27 1642596:1642596 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-23 10:45:27 1642597:1642597 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-23 10:45:27 1642595:1642595 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-23 10:45:27 1642598:1642598 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-23 10:45:27 1642596:1642596 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2024-10-23 10:45:27 1642597:1642597 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2024-10-23 10:45:27 1642595:1642595 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2024-10-23 10:45:27 1642598:1642598 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
gpt2 gpu time 506.8327941894531
STAGE:2024-10-23 10:45:28 1642595:1642595 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
gpt2gpt2gpt2 gpu time  gpu time 506.5695495605469
506.7420654296875
 gpu time 506.7240295410156
STAGE:2024-10-23 10:45:28 1642597:1642597 ActivityProfilerController.cpp:312] Completed Stage: Warm UpSTAGE:2024-10-23 10:45:28 1642596:1642596 ActivityProfilerController.cpp:312] Completed Stage: Warm Up

STAGE:2024-10-23 10:45:28 1642598:1642598 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-10-23 10:45:29 1642598:1642598 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-23 10:45:29 1642595:1642595 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-23 10:45:29 1642596:1642596 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-23 10:45:29 1642597:1642597 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-23 10:45:29 1642598:1642598 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2024-10-23 10:45:29 1642596:1642596 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2024-10-23 10:45:29 1642595:1642595 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2024-10-23 10:45:29 1642597:1642597 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
gpt2gpt2 gpu time 504.0386962890625
 gpu time 504.1239013671875
STAGE:2024-10-23 10:45:29 1642597:1642597 ActivityProfilerController.cpp:312] Completed Stage: Warm UpSTAGE:2024-10-23 10:45:29 1642596:1642596 ActivityProfilerController.cpp:312] Completed Stage: Warm Up

gpt2 gpu time 503.18505859375
STAGE:2024-10-23 10:45:30 1642598:1642598 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
gpt2 gpu time 612.4481201171875
STAGE:2024-10-23 10:45:30 1642595:1642595 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-10-23 10:45:30 1642595:1642595 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-23 10:45:30 1642597:1642597 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-23 10:45:30 1642596:1642596 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-23 10:45:30 1642598:1642598 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-23 10:45:30 1642595:1642595 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2024-10-23 10:45:30 1642597:1642597 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2024-10-23 10:45:30 1642596:1642596 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2024-10-23 10:45:30 1642598:1642598 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
gpt2 gpu time 569.7930297851562
avg profiler Total time1: 531.333627319336
avg Total time2: 489.0351593017578
gpt2 gpu time 504.0551452636719
avg profiler Total time1: 535.8038391113281
avg Total time2: 488.9882568359375
gpt2 gpt2 gpu time gpu time646.9805908203125
avg profiler Total time1: 540.2671752929688
avg Total time2: 489.0521575927734
 646.9017333984375
avg profiler Total time1: 538.8148010253906
avg Total time2: 488.9068603515625
sciml2301:1642598:1642688 [3] NCCL INFO [Service thread] Connection closed by localRank 3
sciml2301:1642595:1642685 [0] NCCL INFO [Service thread] Connection closed by localRank 0
sciml2301:1642596:1642687 [1] NCCL INFO [Service thread] Connection closed by localRank 1
sciml2301:1642597:1642686 [2] NCCL INFO [Service thread] Connection closed by localRank 2
sciml2301:1642598:1642598 [3] NCCL INFO comm 0xf001130 rank 3 nranks 4 cudaDev 3 busId c4000 - Abort COMPLETE
sciml2301:1642595:1642595 [0] NCCL INFO comm 0x13862130 rank 0 nranks 4 cudaDev 0 busId 83000 - Abort COMPLETE
sciml2301:1642596:1642596 [1] NCCL INFO comm 0xe547db0 rank 1 nranks 4 cudaDev 1 busId 84000 - Abort COMPLETE
sciml2301:1642597:1642597 [2] NCCL INFO comm 0x10937790 rank 2 nranks 4 cudaDev 2 busId c3000 - Abort COMPLETE
[2024-10-23 10:45:35,010] torch.distributed.elastic.agent.server.api: [INFO] [default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
[2024-10-23 10:45:35,010] torch.distributed.elastic.agent.server.api: [INFO] Local worker group finished (WorkerState.SUCCEEDED). Waiting 300 seconds for other agents to finish
[2024-10-23 10:45:35,010] torch.distributed.elastic.agent.server.api: [INFO] Done waiting for other agents. Elapsed: 0.0002300739288330078 seconds
