+ env
+ grep -i slurm
SLURM_JOB_USER=xmei
SLURM_TASKS_PER_NODE=1
SLURM_JOB_UID=11066
SLURM_LUSTRE_JOB_ID=sciml2301,xmei,31597520
SLURM_TASK_PID=1724827
SLURM_JOB_GPUS=0,1,2,3
SLURM_LOCALID=0
SLURM_SUBMIT_DIR=/w/epsci-sciwork18/xmei/projects/yifan_sun
SLURMD_NODENAME=sciml2301
SLURM_JOB_START_TIME=1729745024
SLURM_CLUSTER_NAME=scicomp
SLURM_JOB_END_TIME=1729759424
SLURM_CPUS_ON_NODE=4
SLURM_JOB_CPUS_PER_NODE=4
SLURM_GPUS_ON_NODE=4
PRTE_MCA_plm_slurm_args=--external-launcher
SLURM_GTIDS=0
SLURM_JOB_PARTITION=gpu
SLURM_TRES_PER_TASK=cpu:4
SLURM_JOB_NUM_NODES=1
SLURM_JOBID=31597520
SLURM_JOB_QOS=normal
SLURM_PROCID=0
TMPDIR=/scratch/slurm/31597520/.cache/tmp
SLURM_CPUS_PER_TASK=4
SLURM_TOPOLOGY_ADDR=sciml2301
HYDRA_BOOTSTRAP=slurm
SLURM_TOPOLOGY_ADDR_PATTERN=node
SLURM_MEM_PER_CPU=8000
SLURM_SCRIPT_CONTEXT=prolog_task
SLURM_NODELIST=sciml2301
SLURM_JOB_ACCOUNT=epsci
SLURM_PRIO_PROCESS=0
SLURM_NNODES=1
SLURM_SUBMIT_HOST=ifarm2401.jlab.org
XDG_RUNTIME_DIR=/scratch/slurm/31597520/.cache/run
SLURM_JOB_ID=31597520
SLURM_NODEID=0
SLURM_CONF=/etc/slurm/slurm.conf
SLURM_JOB_NAME=1-node_gpt
OMPI_MCA_plm_slurm_args=--external-launcher
SLURM_JOB_GID=761
SLURM_JOB_NODELIST=sciml2301
I_MPI_HYDRA_BOOTSTRAP=slurm
+ env
+ grep -i rank
+ env
+ grep -i cuda
CUDA_VISIBLE_DEVICES=0,1,2,3
+ env
+ grep -i nccl
+ echo -e '=============================================================\n\n'
=============================================================


+ srun --job-name hostname --nodes 1 --ntasks-per-node 1 hostname
sciml2301.jlab.org
++ scontrol show hostnames sciml2301
++ head -n 1
+ export MASTER_ADDR=sciml2301
+ MASTER_ADDR=sciml2301
+ export MASTER_PORT=32800
+ MASTER_PORT=32800
+ echo Head Node: sciml2301:32800
Head Node: sciml2301:32800
+ echo -e '=============================================================\n\n'
=============================================================


+ export PATH=/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/puppetlabs/bin:/home/xmei/projects/iperf3/bin:/home/xmei/projects/py3.10/bin:/work/epsci/xmei/projects/yifan_sun/py-torch/bin
+ PATH=/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/puppetlabs/bin:/home/xmei/projects/iperf3/bin:/home/xmei/projects/py3.10/bin:/work/epsci/xmei/projects/yifan_sun/py-torch/bin
+ export LOGLEVEL=INFO
+ LOGLEVEL=INFO
+ export NCCL_DEBUG=INFO
+ NCCL_DEBUG=INFO
+ export TORCH_DISTRIBUTED_DEBUG=INFO
+ TORCH_DISTRIBUTED_DEBUG=INFO
+ export CUDA_VISIBLE_DEVICES=0,1,2,3
+ CUDA_VISIBLE_DEVICES=0,1,2,3
+ export 'PYTHON_DIST_JOB_ARGS=-m torch.distributed.run --nproc_per_node=4 --nnodes=1 --master-addr  --master-port '
+ PYTHON_DIST_JOB_ARGS='-m torch.distributed.run --nproc_per_node=4 --nnodes=1 --master-addr  --master-port '
+ srun --job-name print-cuda --nodes 1 --ntasks-per-node 1 '--wrap=echo $(hostname), echo "CUDA_DEV: ${CUDA_VISIBLE_DEVICES}"'
srun: unrecognized option '--wrap=echo $(hostname), echo "CUDA_DEV: ${CUDA_VISIBLE_DEVICES}"'
Try "srun --help" for more information
+ echo -e '=============================================================\n\n'
=============================================================


+ srun torchrun --nproc_per_node=4 --rdzv_backend=c10d --rdzv_endpoint=sciml2301.jlab.org:32800 --nnodes=1 --rdzv-id 15512 transformer_ddp.py 512

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.1.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/bin/torchrun", line 5, in <module>
    from torch.distributed.run import main
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/__init__.py", line 1382, in <module>
    from .functional import *  # noqa: F403
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/functional.py", line 7, in <module>
    import torch.nn.functional as F
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/nn/__init__.py", line 1, in <module>
    from .modules import *  # noqa: F403
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/nn/modules/__init__.py", line 35, in <module>
    from .transformer import TransformerEncoder, TransformerDecoder, \
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/nn/modules/transformer.py", line 20, in <module>
    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)
  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
[2024-10-24 00:43:46,084] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[2024-10-24 00:43:46,085] torch.distributed.run: [WARNING] 
[2024-10-24 00:43:46,085] torch.distributed.run: [WARNING] *****************************************
[2024-10-24 00:43:46,085] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-10-24 00:43:46,085] torch.distributed.run: [WARNING] *****************************************
[2024-10-24 00:43:46,085] torch.distributed.launcher.api: [INFO] Starting elastic_operator with launch configs:
[2024-10-24 00:43:46,085] torch.distributed.launcher.api: [INFO]   entrypoint       : transformer_ddp.py
[2024-10-24 00:43:46,085] torch.distributed.launcher.api: [INFO]   min_nodes        : 1
[2024-10-24 00:43:46,085] torch.distributed.launcher.api: [INFO]   max_nodes        : 1
[2024-10-24 00:43:46,085] torch.distributed.launcher.api: [INFO]   nproc_per_node   : 4
[2024-10-24 00:43:46,085] torch.distributed.launcher.api: [INFO]   run_id           : 15512
[2024-10-24 00:43:46,085] torch.distributed.launcher.api: [INFO]   rdzv_backend     : c10d
[2024-10-24 00:43:46,085] torch.distributed.launcher.api: [INFO]   rdzv_endpoint    : sciml2301.jlab.org:32800
[2024-10-24 00:43:46,085] torch.distributed.launcher.api: [INFO]   rdzv_configs     : {'timeout': 900}
[2024-10-24 00:43:46,085] torch.distributed.launcher.api: [INFO]   max_restarts     : 0
[2024-10-24 00:43:46,085] torch.distributed.launcher.api: [INFO]   monitor_interval : 5
[2024-10-24 00:43:46,085] torch.distributed.launcher.api: [INFO]   log_dir          : None
[2024-10-24 00:43:46,085] torch.distributed.launcher.api: [INFO]   metrics_cfg      : {}
[2024-10-24 00:43:46,085] torch.distributed.launcher.api: [INFO] 
[2024-10-24 00:43:46,104] torch.distributed.elastic.agent.server.local_elastic_agent: [INFO] log directory set to: /scratch/slurm/31597520/.cache/tmp/torchelastic_2mhjpb8b/15512_8j_soo_1
[2024-10-24 00:43:46,104] torch.distributed.elastic.agent.server.api: [INFO] [default] starting workers for entrypoint: python3.10
[2024-10-24 00:43:46,104] torch.distributed.elastic.agent.server.api: [INFO] [default] Rendezvous'ing worker group
[2024-10-24 00:43:46,336] torch.distributed.elastic.agent.server.api: [INFO] [default] Rendezvous complete for workers. Result:
[2024-10-24 00:43:46,336] torch.distributed.elastic.agent.server.api: [INFO]   restart_count=0
[2024-10-24 00:43:46,336] torch.distributed.elastic.agent.server.api: [INFO]   master_addr=sciml2301.jlab.org
[2024-10-24 00:43:46,336] torch.distributed.elastic.agent.server.api: [INFO]   master_port=54287
[2024-10-24 00:43:46,336] torch.distributed.elastic.agent.server.api: [INFO]   group_rank=0
[2024-10-24 00:43:46,336] torch.distributed.elastic.agent.server.api: [INFO]   group_world_size=1
[2024-10-24 00:43:46,336] torch.distributed.elastic.agent.server.api: [INFO]   local_ranks=[0, 1, 2, 3]
[2024-10-24 00:43:46,336] torch.distributed.elastic.agent.server.api: [INFO]   role_ranks=[0, 1, 2, 3]
[2024-10-24 00:43:46,336] torch.distributed.elastic.agent.server.api: [INFO]   global_ranks=[0, 1, 2, 3]
[2024-10-24 00:43:46,336] torch.distributed.elastic.agent.server.api: [INFO]   role_world_sizes=[4, 4, 4, 4]
[2024-10-24 00:43:46,336] torch.distributed.elastic.agent.server.api: [INFO]   global_world_sizes=[4, 4, 4, 4]
[2024-10-24 00:43:46,336] torch.distributed.elastic.agent.server.api: [INFO] 
[2024-10-24 00:43:46,336] torch.distributed.elastic.agent.server.api: [INFO] [default] Starting worker group
[2024-10-24 00:43:46,336] torch.distributed.elastic.agent.server.local_elastic_agent: [INFO] Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
[2024-10-24 00:43:46,336] torch.distributed.elastic.multiprocessing: [INFO] Setting worker0 reply file to: /scratch/slurm/31597520/.cache/tmp/torchelastic_2mhjpb8b/15512_8j_soo_1/attempt_0/0/error.json
[2024-10-24 00:43:46,336] torch.distributed.elastic.multiprocessing: [INFO] Setting worker1 reply file to: /scratch/slurm/31597520/.cache/tmp/torchelastic_2mhjpb8b/15512_8j_soo_1/attempt_0/1/error.json
[2024-10-24 00:43:46,336] torch.distributed.elastic.multiprocessing: [INFO] Setting worker2 reply file to: /scratch/slurm/31597520/.cache/tmp/torchelastic_2mhjpb8b/15512_8j_soo_1/attempt_0/2/error.json
[2024-10-24 00:43:46,336] torch.distributed.elastic.multiprocessing: [INFO] Setting worker3 reply file to: /scratch/slurm/31597520/.cache/tmp/torchelastic_2mhjpb8b/15512_8j_soo_1/attempt_0/3/error.json

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.1.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/transformer_ddp.py", line 4, in <module>
    import torch
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/__init__.py", line 1382, in <module>
    from .functional import *  # noqa: F403
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/functional.py", line 7, in <module>
    import torch.nn.functional as F
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/nn/__init__.py", line 1, in <module>
    from .modules import *  # noqa: F403
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/nn/modules/__init__.py", line 35, in <module>
    from .transformer import TransformerEncoder, TransformerDecoder, \
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/nn/modules/transformer.py", line 20, in <module>
    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.1.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/transformer_ddp.py", line 4, in <module>
    import torch
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/__init__.py", line 1382, in <module>
    from .functional import *  # noqa: F403
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/functional.py", line 7, in <module>
    import torch.nn.functional as F
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/nn/__init__.py", line 1, in <module>
    from .modules import *  # noqa: F403
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/nn/modules/__init__.py", line 35, in <module>
    from .transformer import TransformerEncoder, TransformerDecoder, \
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/nn/modules/transformer.py", line 20, in <module>
    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)
  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)
  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.1.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/transformer_ddp.py", line 4, in <module>
    import torch
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/__init__.py", line 1382, in <module>
    from .functional import *  # noqa: F403
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/functional.py", line 7, in <module>
    import torch.nn.functional as F
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/nn/__init__.py", line 1, in <module>
    from .modules import *  # noqa: F403
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/nn/modules/__init__.py", line 35, in <module>
    from .transformer import TransformerEncoder, TransformerDecoder, \
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/nn/modules/transformer.py", line 20, in <module>
    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)
  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.1.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/transformer_ddp.py", line 4, in <module>
    import torch
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/__init__.py", line 1382, in <module>
    from .functional import *  # noqa: F403
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/functional.py", line 7, in <module>
    import torch.nn.functional as F
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/nn/__init__.py", line 1, in <module>
    from .modules import *  # noqa: F403
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/nn/modules/__init__.py", line 35, in <module>
    from .transformer import TransformerEncoder, TransformerDecoder, \
  File "/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/nn/modules/transformer.py", line 20, in <module>
    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
/w/epsci-sciwork18/xmei/projects/yifan_sun/py-torch/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)
  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
Hostname: sciml2301.jlab.org, Rank: 0, Local Rank: 0, Global Rank: 0, NUM_GPS: 4
Hostname: sciml2301.jlab.org, Rank: 1, Local Rank: 1, Global Rank: 1, NUM_GPS: 4
Hostname: sciml2301.jlab.org, Rank: 2, Local Rank: 2, Global Rank: 2, NUM_GPS: 4
[sciml2301.jlab.org] Rank 0, Local Rank 0: CUDA device set to 0
Hostname: sciml2301.jlab.org, Rank: 3, Local Rank: 3, Global Rank: 3, NUM_GPS: 4
[sciml2301.jlab.org] Rank 2, Local Rank 2: CUDA device set to 2
[sciml2301.jlab.org] Rank 3, Local Rank 3: CUDA device set to 3
[sciml2301.jlab.org] Rank 1, Local Rank 1: CUDA device set to 1
Dataset loaded.
training model now: gpt2
Dataset loaded.
training model now: gpt2
Dataset loaded.
training model now: gpt2
Dataset loaded.
training model now: gpt2
sciml2301:1724882:1724882 [0] NCCL INFO Bootstrap : Using ibp37s0:172.17.1.12<0>
sciml2301:1724882:1724882 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
sciml2301:1724882:1724882 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
sciml2301:1724882:1724882 [0] NCCL INFO cudaDriverVersion 12060
NCCL version 2.18.1+cuda12.1
sciml2301:1724885:1724885 [3] NCCL INFO cudaDriverVersion 12060
sciml2301:1724885:1724885 [3] NCCL INFO Bootstrap : Using ibp37s0:172.17.1.12<0>
sciml2301:1724885:1724885 [3] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
sciml2301:1724885:1724885 [3] NCCL INFO NET/Plugin : No plugin found, using internal implementation
sciml2301:1724884:1724884 [2] NCCL INFO cudaDriverVersion 12060
sciml2301:1724884:1724884 [2] NCCL INFO Bootstrap : Using ibp37s0:172.17.1.12<0>
sciml2301:1724884:1724884 [2] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
sciml2301:1724884:1724884 [2] NCCL INFO NET/Plugin : No plugin found, using internal implementation
sciml2301:1724883:1724883 [1] NCCL INFO cudaDriverVersion 12060
sciml2301:1724883:1724883 [1] NCCL INFO Bootstrap : Using ibp37s0:172.17.1.12<0>
sciml2301:1724883:1724883 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
sciml2301:1724883:1724883 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
sciml2301:1724882:1724966 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibp37s0:172.17.1.12<0>
sciml2301:1724882:1724966 [0] NCCL INFO Using network IB
sciml2301:1724882:1724966 [0] NCCL INFO DMA-BUF is available on GPU device 0
sciml2301:1724885:1724967 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibp37s0:172.17.1.12<0>
sciml2301:1724885:1724967 [3] NCCL INFO Using network IB
sciml2301:1724885:1724967 [3] NCCL INFO DMA-BUF is available on GPU device 3
sciml2301:1724884:1724968 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibp37s0:172.17.1.12<0>
sciml2301:1724884:1724968 [2] NCCL INFO Using network IB
sciml2301:1724884:1724968 [2] NCCL INFO DMA-BUF is available on GPU device 2
sciml2301:1724883:1724969 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibp37s0:172.17.1.12<0>
sciml2301:1724883:1724969 [1] NCCL INFO Using network IB
sciml2301:1724883:1724969 [1] NCCL INFO DMA-BUF is available on GPU device 1
sciml2301:1724883:1724969 [1] NCCL INFO NVLS multicast support is not available on dev 1
sciml2301:1724884:1724968 [2] NCCL INFO NVLS multicast support is not available on dev 2
sciml2301:1724885:1724967 [3] NCCL INFO NVLS multicast support is not available on dev 3
sciml2301:1724882:1724966 [0] NCCL INFO NVLS multicast support is not available on dev 0
sciml2301:1724885:1724967 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
sciml2301:1724885:1724967 [3] NCCL INFO P2P Chunksize set to 524288
sciml2301:1724882:1724966 [0] NCCL INFO Channel 00/02 :    0   1   2   3
sciml2301:1724882:1724966 [0] NCCL INFO Channel 01/02 :    0   1   2   3
sciml2301:1724883:1724969 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
sciml2301:1724882:1724966 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
sciml2301:1724883:1724969 [1] NCCL INFO P2P Chunksize set to 524288
sciml2301:1724882:1724966 [0] NCCL INFO P2P Chunksize set to 524288
sciml2301:1724884:1724968 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
sciml2301:1724884:1724968 [2] NCCL INFO P2P Chunksize set to 524288
sciml2301:1724882:1724966 [0] NCCL INFO Channel 00/0 : 0[83000] -> 1[84000] via P2P/IPC/read
sciml2301:1724883:1724969 [1] NCCL INFO Channel 00/0 : 1[84000] -> 2[c3000] via P2P/IPC
sciml2301:1724885:1724967 [3] NCCL INFO Channel 00/0 : 3[c4000] -> 0[83000] via P2P/IPC
sciml2301:1724884:1724968 [2] NCCL INFO Channel 00/0 : 2[c3000] -> 3[c4000] via P2P/IPC/read
sciml2301:1724882:1724966 [0] NCCL INFO Channel 01/0 : 0[83000] -> 1[84000] via P2P/IPC/read
sciml2301:1724883:1724969 [1] NCCL INFO Channel 01/0 : 1[84000] -> 2[c3000] via P2P/IPC
sciml2301:1724884:1724968 [2] NCCL INFO Channel 01/0 : 2[c3000] -> 3[c4000] via P2P/IPC/read
sciml2301:1724885:1724967 [3] NCCL INFO Channel 01/0 : 3[c4000] -> 0[83000] via P2P/IPC
sciml2301:1724883:1724969 [1] NCCL INFO Connected all rings
sciml2301:1724884:1724968 [2] NCCL INFO Connected all rings
sciml2301:1724882:1724966 [0] NCCL INFO Connected all rings
sciml2301:1724885:1724967 [3] NCCL INFO Connected all rings
sciml2301:1724885:1724967 [3] NCCL INFO Channel 00/0 : 3[c4000] -> 2[c3000] via P2P/IPC/read
sciml2301:1724885:1724967 [3] NCCL INFO Channel 01/0 : 3[c4000] -> 2[c3000] via P2P/IPC/read
sciml2301:1724884:1724968 [2] NCCL INFO Channel 00/0 : 2[c3000] -> 1[84000] via P2P/IPC
sciml2301:1724883:1724969 [1] NCCL INFO Channel 00/0 : 1[84000] -> 0[83000] via P2P/IPC/read
sciml2301:1724884:1724968 [2] NCCL INFO Channel 01/0 : 2[c3000] -> 1[84000] via P2P/IPC
sciml2301:1724883:1724969 [1] NCCL INFO Channel 01/0 : 1[84000] -> 0[83000] via P2P/IPC/read
sciml2301:1724885:1724967 [3] NCCL INFO Connected all trees
sciml2301:1724885:1724967 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
sciml2301:1724885:1724967 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
sciml2301:1724883:1724969 [1] NCCL INFO Connected all trees
sciml2301:1724882:1724966 [0] NCCL INFO Connected all trees
sciml2301:1724883:1724969 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
sciml2301:1724883:1724969 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
sciml2301:1724882:1724966 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
sciml2301:1724882:1724966 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
sciml2301:1724884:1724968 [2] NCCL INFO Connected all trees
sciml2301:1724884:1724968 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
sciml2301:1724884:1724968 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
sciml2301:1724882:1724966 [0] NCCL INFO comm 0xd8bb400 rank 0 nranks 4 cudaDev 0 busId 83000 commId 0x471be02cf427c13f - Init COMPLETE
sciml2301:1724884:1724968 [2] NCCL INFO comm 0xd0d2af0 rank 2 nranks 4 cudaDev 2 busId c3000 commId 0x471be02cf427c13f - Init COMPLETE
sciml2301:1724885:1724967 [3] NCCL INFO comm 0x106800d0 rank 3 nranks 4 cudaDev 3 busId c4000 commId 0x471be02cf427c13f - Init COMPLETE
sciml2301:1724883:1724969 [1] NCCL INFO comm 0x10213770 rank 1 nranks 4 cudaDev 1 busId 84000 commId 0x471be02cf427c13f - Init COMPLETE
[sciml2301.jlab.org] Rank 0, Local Rank 0: gpt2, gpu time 2: 913.5452270507812[sciml2301.jlab.org] Rank 3, Local Rank 3: gpt2, gpu time 2: 913.64453125
[sciml2301.jlab.org] Rank 1, Local Rank 1: gpt2, gpu time 2: 913.597412109375[sciml2301.jlab.org] Rank 2, Local Rank 2: gpt2, gpu time 2: 913.5994873046875


[sciml2301.jlab.org] Rank 0, Local Rank 0: gpt2, gpu time 2: 914.8159790039062[sciml2301.jlab.org] Rank 3, Local Rank 3: gpt2, gpu time 2: 915.030029296875
[sciml2301.jlab.org] Rank 2, Local Rank 2: gpt2, gpu time 2: 915.0208129882812
[sciml2301.jlab.org] Rank 1, Local Rank 1: gpt2, gpu time 2: 915.0115966796875

[sciml2301.jlab.org] Rank 0, Local Rank 0: gpt2, gpu time 2: 911.510498046875[sciml2301.jlab.org] Rank 3, Local Rank 3: gpt2, gpu time 2: 911.6917724609375
[sciml2301.jlab.org] Rank 2, Local Rank 2: gpt2, gpu time 2: 911.7183837890625
[sciml2301.jlab.org] Rank 1, Local Rank 1: gpt2, gpu time 2: 911.7276000976562

[sciml2301.jlab.org] Rank 3, Local Rank 3: gpt2, gpu time 2: 916.57421875[sciml2301.jlab.org] Rank 1, Local Rank 1: gpt2, gpu time 2: 916.5629272460938
[sciml2301.jlab.org] Rank 0, Local Rank 0: gpt2, gpu time 2: 916.3704223632812
[sciml2301.jlab.org] Rank 2, Local Rank 2: gpt2, gpu time 2: 916.6264038085938

[sciml2301.jlab.org] Rank 3, Local Rank 3: gpt2, gpu time 2: 911.5699462890625[sciml2301.jlab.org] Rank 0, Local Rank 0: gpt2, gpu time 2: 911.6282958984375
[sciml2301.jlab.org] Rank 1, Local Rank 1: gpt2, gpu time 2: 911.7378540039062[sciml2301.jlab.org] Rank 2, Local Rank 2: gpt2, gpu time 2: 911.6958618164062


[sciml2301.jlab.org] Rank 0, Local Rank 0: gpt2, gpu time 2: 911.3118896484375[sciml2301.jlab.org] Rank 3, Local Rank 3: gpt2, gpu time 2: 911.2484130859375
[sciml2301.jlab.org] Rank 1, Local Rank 1: gpt2, gpu time 2: 911.3681640625
[sciml2301.jlab.org] Rank 2, Local Rank 2: gpt2, gpu time 2: 911.3825073242188

[sciml2301.jlab.org] Rank 0, Local Rank 0: gpt2, gpu time 2: 911.3753662109375[sciml2301.jlab.org] Rank 3, Local Rank 3: gpt2, gpu time 2: 911.6282958984375
[sciml2301.jlab.org] Rank 1, Local Rank 1: gpt2, gpu time 2: 911.6528930664062
[sciml2301.jlab.org] Rank 2, Local Rank 2: gpt2, gpu time 2: 911.6456909179688

[sciml2301.jlab.org] Rank 0, Local Rank 0: gpt2, gpu time 2: 911.37841796875[sciml2301.jlab.org] Rank 2, Local Rank 2: gpt2, gpu time 2: 911.5648193359375
[sciml2301.jlab.org] Rank 3, Local Rank 3: gpt2, gpu time 2: 911.5903930664062
[sciml2301.jlab.org] Rank 1, Local Rank 1: gpt2, gpu time 2: 911.6077880859375

[sciml2301.jlab.org] Rank 0, Local Rank 0: gpt2, gpu time 2: 913.375244140625
[sciml2301.jlab.org] Rank 3, Local Rank 3: gpt2, gpu time 2: 913.5820922851562
[sciml2301.jlab.org] Rank 1, Local Rank 1: gpt2, gpu time 2: 913.5523681640625
[sciml2301.jlab.org] Rank 2, Local Rank 2: gpt2, gpu time 2: 913.6434936523438
[sciml2301.jlab.org] Rank 0, Local Rank 0: gpt2, gpu time 2: 913.5677490234375[sciml2301.jlab.org] Rank 1, Local Rank 1: gpt2, gpu time 2: 913.4581909179688
[sciml2301.jlab.org] Rank 3, Local Rank 3: gpt2, gpu time 2: 913.5482788085938
[sciml2301.jlab.org] Rank 2, Local Rank 2: gpt2, gpu time 2: 913.4192504882812

STAGE:2024-10-24 00:44:33 1724882:1724882 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-10-24 00:44:33 1724884:1724884 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-10-24 00:44:33 1724885:1724885 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-10-24 00:44:33 1724883:1724883 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-10-24 00:44:34 1724885:1724885 ActivityProfilerController.cpp:318] Completed Stage: CollectionSTAGE:2024-10-24 00:44:34 1724884:1724884 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-24 00:44:34 1724883:1724883 ActivityProfilerController.cpp:318] Completed Stage: Collection

STAGE:2024-10-24 00:44:34 1724882:1724882 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-24 00:44:34 1724883:1724883 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2024-10-24 00:44:34 1724884:1724884 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2024-10-24 00:44:34 1724885:1724885 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2024-10-24 00:44:34 1724882:1724882 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
[sciml2301.jlab.org] Rank 0, Local Rank 0: gpt2, gpu time: 933.2388305664062
[sciml2301.jlab.org] Rank 2, Local Rank 2: gpt2, gpu time: 930.6854248046875
STAGE:2024-10-24 00:44:35 1724884:1724884 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-10-24 00:44:35 1724882:1724882 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
[sciml2301.jlab.org] Rank 3, Local Rank 3: gpt2, gpu time: 925.86181640625[sciml2301.jlab.org] Rank 1, Local Rank 1: gpt2, gpu time: 925.1980590820312

STAGE:2024-10-24 00:44:35 1724883:1724883 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-10-24 00:44:35 1724885:1724885 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-10-24 00:44:36 1724882:1724882 ActivityProfilerController.cpp:318] Completed Stage: CollectionSTAGE:2024-10-24 00:44:36 1724883:1724883 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-24 00:44:36 1724885:1724885 ActivityProfilerController.cpp:318] Completed Stage: Collection

STAGE:2024-10-24 00:44:36 1724884:1724884 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-24 00:44:36 1724883:1724883 ActivityProfilerController.cpp:322] Completed Stage: Post ProcessingSTAGE:2024-10-24 00:44:36 1724885:1724885 ActivityProfilerController.cpp:322] Completed Stage: Post Processing

STAGE:2024-10-24 00:44:36 1724882:1724882 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2024-10-24 00:44:36 1724884:1724884 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
[sciml2301.jlab.org] Rank 1, Local Rank 1: gpt2, gpu time: 928.4013061523438[sciml2301.jlab.org] Rank 3, Local Rank 3: gpt2, gpu time: 927.2744750976562

STAGE:2024-10-24 00:44:37 1724883:1724883 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-10-24 00:44:37 1724885:1724885 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
[sciml2301.jlab.org] Rank 2, Local Rank 2: gpt2, gpu time: 1049.0941162109375[sciml2301.jlab.org] Rank 0, Local Rank 0: gpt2, gpu time: 1049.0235595703125

STAGE:2024-10-24 00:44:37 1724884:1724884 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-10-24 00:44:37 1724882:1724882 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-10-24 00:44:38 1724885:1724885 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-24 00:44:38 1724883:1724883 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-24 00:44:38 1724882:1724882 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-24 00:44:38 1724885:1724885 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2024-10-24 00:44:38 1724882:1724882 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2024-10-24 00:44:38 1724884:1724884 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-24 00:44:38 1724883:1724883 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2024-10-24 00:44:38 1724884:1724884 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
[sciml2301.jlab.org] Rank 0, Local Rank 0: gpt2, gpu time: 934.9055786132812
STAGE:2024-10-24 00:44:39 1724882:1724882 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
[sciml2301.jlab.org] Rank 2, Local Rank 2: gpt2, gpu time: 922.67041015625
[sciml2301.jlab.org] Rank 1, Local Rank 1: gpt2, gpu time: 1101.6456298828125
STAGE:2024-10-24 00:44:39 1724884:1724884 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-10-24 00:44:39 1724883:1724883 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
[sciml2301.jlab.org] Rank 3, Local Rank 3: gpt2, gpu time: 1101.7476806640625
STAGE:2024-10-24 00:44:39 1724885:1724885 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-10-24 00:44:40 1724883:1724883 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-24 00:44:40 1724884:1724884 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-24 00:44:40 1724882:1724882 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-24 00:44:40 1724883:1724883 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2024-10-24 00:44:40 1724885:1724885 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-24 00:44:40 1724884:1724884 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2024-10-24 00:44:40 1724882:1724882 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2024-10-24 00:44:40 1724885:1724885 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
[sciml2301.jlab.org] Rank 1, Local Rank 1: gpt2, gpu time: 967.734375[sciml2301.jlab.org] Rank 3, Local Rank 3: gpt2, gpu time: 921.88232421875

STAGE:2024-10-24 00:44:41 1724885:1724885 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-10-24 00:44:41 1724883:1724883 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
[sciml2301.jlab.org] Rank 2, Local Rank 2: gpt2, gpu time: 968.72900390625[sciml2301.jlab.org] Rank 0, Local Rank 0: gpt2, gpu time: 1047.7740478515625

STAGE:2024-10-24 00:44:41 1724884:1724884 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-10-24 00:44:41 1724882:1724882 ActivityProfilerController.cpp:312] Completed Stage: Warm Up
STAGE:2024-10-24 00:44:42 1724884:1724884 ActivityProfilerController.cpp:318] Completed Stage: CollectionSTAGE:2024-10-24 00:44:42 1724882:1724882 ActivityProfilerController.cpp:318] Completed Stage: Collection

STAGE:2024-10-24 00:44:42 1724883:1724883 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-24 00:44:42 1724885:1724885 ActivityProfilerController.cpp:318] Completed Stage: Collection
STAGE:2024-10-24 00:44:42 1724884:1724884 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2024-10-24 00:44:42 1724882:1724882 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2024-10-24 00:44:42 1724883:1724883 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
STAGE:2024-10-24 00:44:42 1724885:1724885 ActivityProfilerController.cpp:322] Completed Stage: Post Processing
[sciml2301.jlab.org] Rank 2, Local Rank 2: gpt2, gpu time: 925.2854614257812
[sciml2301.jlab.org] Rank 2, Local Rank 2: avg profiler Total time1: 959.2928833007812
[sciml2301.jlab.org] Rank 2, Local Rank 2: avg Total time2: 1826.0633422851563
[sciml2301.jlab.org] Rank 0, Local Rank 0: gpt2, gpu time: 925.0880126953125
[sciml2301.jlab.org] Rank 0, Local Rank 0: avg profiler Total time1: 978.006005859375
[sciml2301.jlab.org] Rank 0, Local Rank 0: avg Total time2: 1825.7758178710938
[sciml2301.jlab.org] Rank 3, Local Rank 3: gpt2, gpu time: 1062.8802490234375
[sciml2301.jlab.org] Rank 3, Local Rank 3: avg profiler Total time1: 987.9293090820313
[sciml2301.jlab.org] Rank 3, Local Rank 3: avg Total time2: 1826.0215942382813
[sciml2301.jlab.org] Rank 1, Local Rank 1: gpt2, gpu time: 1063.2674560546875
[sciml2301.jlab.org] Rank 1, Local Rank 1: avg profiler Total time1: 997.249365234375
[sciml2301.jlab.org] Rank 1, Local Rank 1: avg Total time2: 1826.0553588867188
sciml2301:1724884:1724977 [2] NCCL INFO [Service thread] Connection closed by localRank 2
sciml2301:1724882:1724975 [0] NCCL INFO [Service thread] Connection closed by localRank 0
sciml2301:1724885:1724976 [3] NCCL INFO [Service thread] Connection closed by localRank 3
sciml2301:1724883:1724974 [1] NCCL INFO [Service thread] Connection closed by localRank 1
sciml2301:1724884:1724884 [2] NCCL INFO comm 0xd0d2af0 rank 2 nranks 4 cudaDev 2 busId c3000 - Abort COMPLETE
sciml2301:1724882:1724882 [0] NCCL INFO comm 0xd8bb400 rank 0 nranks 4 cudaDev 0 busId 83000 - Abort COMPLETE
sciml2301:1724885:1724885 [3] NCCL INFO comm 0x106800d0 rank 3 nranks 4 cudaDev 3 busId c4000 - Abort COMPLETE
sciml2301:1724883:1724883 [1] NCCL INFO comm 0x10213770 rank 1 nranks 4 cudaDev 1 busId 84000 - Abort COMPLETE
[2024-10-24 00:44:46,400] torch.distributed.elastic.agent.server.api: [INFO] [default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
[2024-10-24 00:44:46,400] torch.distributed.elastic.agent.server.api: [INFO] Local worker group finished (WorkerState.SUCCEEDED). Waiting 300 seconds for other agents to finish
[2024-10-24 00:44:46,401] torch.distributed.elastic.agent.server.api: [INFO] Done waiting for other agents. Elapsed: 0.00023436546325683594 seconds
